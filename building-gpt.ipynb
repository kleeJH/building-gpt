{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under input/tinyshakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get data using pywget\n",
    "!python -m wget -o input/tinyshakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total character length: 1115394\n",
      "\n",
      "\n",
      "First 500 characters:\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "\n",
      "\n",
      "Unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Total unique characters: 65 characters\n"
     ]
    }
   ],
   "source": [
    "# Read and inspect text file\n",
    "with open('./input/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    texts = f.read()\n",
    "\n",
    "# Check length of characters\n",
    "print('Total character length: {0}\\n\\n'.format(len(texts)))\n",
    "\n",
    "# Example texts\n",
    "print('First 500 characters:\\n\\n{0}\\n\\n'.format(texts[:500]))\n",
    "\n",
    "# Unique characters\n",
    "chars = sorted(list(set(texts)))\n",
    "print('Unique characters: {0}'.format(''.join(chars)))\n",
    "print('Total unique characters: {0} characters'.format(len(chars)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Tokenization\n",
    "I will be using a simple character to index tokenizer.<br/>\n",
    "More complex GPTs uses tokenizer on a word/sub-word level.<br/>\n",
    "So tokenizer packages we can use are `tiktoken` from OpenAI or `SentencePiece` from Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Words: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 2]\n",
      "Decoded Words: Hello World!\n"
     ]
    }
   ],
   "source": [
    "# To make it simple, encoding and decode will be done at a character level\n",
    "str_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_str = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "# encode & decode function\n",
    "encode = lambda s: [str_to_idx[ch] for ch in s]\n",
    "decode = lambda idx: \"\".join([idx_to_str[i] for i in idx])\n",
    "\n",
    "# Example\n",
    "print('Encoded Words: {0}'.format(encode(\"Hello World!\")))\n",
    "print('Decoded Words: {0}'.format(decode(encode(\"Hello World!\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Availability: True\n",
      "Shape: torch.Size([1115394])\n",
      "Data Type:torch.int64\n",
      "First 500 encoded characters:\n",
      "\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('CUDA Availability: {0}'.format(torch.cuda.is_available()))\n",
    "\n",
    "data = torch.tensor(encode(texts), dtype=torch.long)\n",
    "print('Shape: {0}\\nData Type:{1}'.format(data.shape, data.dtype))\n",
    "print('First 500 encoded characters:\\n\\n{0}\\n\\n'.format(data[:500]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Explaining using a Bigram Language Model\n",
    "Check out `no-self-attenton-simple-bigram.py` to see the process of training a GPT using a simple language model.<br /><br />\n",
    "\n",
    "A bigram language model is a type of statistical language model that predicts the probability of a word in a sequence based on the previous word. It considers pairs of consecutive words (bigrams) and estimates the likelihood of encountering a specific word given the preceding word in a text or sentence.<br /><br />\n",
    "\n",
    "This model does not have any self-attention, meaning that it is unable to capture dependencies between tokens and their relationships within a set of input.<br /><br />\n",
    "\n",
    "Example: Input -> \"The fox jumps over the lazy \", Prediction -> \"The fox jumps over the lazy `dog`\"<br />\n",
    "In this case, the input is usually encoded and is converted into an embedding vector using an Embedding layer.<br />\n",
    "Depending on the level of granularity/detail, the model could predict in the level of characters, sub-words or word/s.<br /><br />\n",
    "\n",
    "Shout out to [Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy/featured) for the indescribable introduction to GPT."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Attention is all you need\n",
    "Check out `decoder-transformer.py` to see the process of training a GPT using larger language model.<br /><br />\n",
    "\n",
    "### Positional Encoding\n",
    "Before we get into attention, we need to address a specific issue that is present in our model.\n",
    "Our model does not know the information about the position of tokens in a sequence.\n",
    "So, Transformers do not inherently have a sense of order for the elements in a sequence since they process the entire sequence in parallel.\n",
    "To solve this, we use an embedding table to encode the positional values of a set of token input sequence.\n",
    "As a result, the Transformer can capture sequential information and relationships between different positions in the input sequence, making it suitable for tasks like language translation and language modeling.\n",
    "This is called `positional encoding`.\n",
    "\n",
    "Then, we add the token embeddings and the positional encoding to produce an input that has information about each token relationship to other tokens and their positions.\n",
    "\n",
    "### Attention\n",
    "We need some sort of way for the model to get a grasp of the past history.\n",
    "And as well, have a way to gauge how important certain tokens (within a subset of samples) are relevant to predicting the next token.\n",
    "\n",
    "This is where the `Self-Attention` head come into play.\n",
    "Using the research from [Attention Is All You Need](https://arxiv.org/abs/1706.03762), Scaled Dot-Product Attention allows for the conditions above to be met.\n",
    "\n",
    "But, a single self-attention head is not enough to extract the intricate relationships between tokens.\n",
    "Thus, using a multiple of the self-attention head could be use to jointly attend to information from different representation subspaces at different positions (which couldn't be achieve by a single head).\n",
    "\n",
    "### Model\n",
    "Now, we would have the transformer block looking like this:\n",
    "\n",
    "- LayerNorm\n",
    "- MultiHeadAttention (Masked inputs, which makes it a decoder)\n",
    "- Add(x)\n",
    "- LayerNorm\n",
    "- FeedForward\n",
    "- Add(x)\n",
    "\n",
    "Note: LayerNorm is used instead of BatchNorm in NLP tasks. Layer normalization normalizes input across the features instead of normalizing input features across the batch dimension in batch normalization. And in this case the features are the size of each token input sequence. It helps with bias, normalizes values and reduce internal covariate shift (variables manipulation other variables).\n",
    "\n",
    "### Scaling Up\n",
    "To further scale-up the model and to reduce the validation loss, we can scale up the model by repeating the transformer blocks multiple times. But, this will introduce `overfitting` to occur. Scaling up the model can also lead to unwanted effects such as overfitting, we can use `regularization` techniques such as Dropout to try to prevent those effects.<br /><br /> \n",
    "Dropout decativates neurons (turn gradients to zero) can produce effects similar to the decoder transformer blocks that mask certain inputs so that the predictions do not rely on it. But in this case, turns of a percentage of the neurons in random positions and it reduce the chances for neurons to be co-adapting to other neurons.\n",
    "\n",
    "### Full Model\n",
    "- Add(TokenEmbedding(x), PositionalEmbedding(x))\n",
    "- multiple TransformerBlocks\n",
    "- LayerNorm\n",
    "- Linear\n",
    "\n",
    "### Final\n",
    "In the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762), there is an encoder and a decoder transformer block. The difference between encoder and decoder is if the multi self-attention blocks have masked inputs or not.\n",
    "\n",
    "Our model does not use the encoder transformer block because the paper is specifically written to do language translation. The encoder block originally is used to train the language it wants to translate and find all the intricate details such as the relationship and postion of tokens. So, it does not need provide masked inputs.\n",
    "\n",
    "However, decoder requires a way for the model to actually learn and understand how to translate the encoded language into the decoded language. So, it mask the inputs and the model is require to predict tokens by not being able to see future tokens.\n",
    "\n",
    "Finally, in the proposed architecture by the research paper, there is a component called the cross-attention head. Language translation requires the model to understand how to translate from x to y. You need a way for the model to combine both the encoded and initial decoded parts into one. It allows NLP models to capture intricate relationships and dependencies between different input sequences. Self-attention focuses on understanding it's own token input sequence. In a way, cross-attention enables a way for the model to assimulate information from multiple sources of data effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
